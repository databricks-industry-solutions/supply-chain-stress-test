{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e31a53f9-a581-4146-9f08-e7dd5b151c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluate Supply Chain Agent\n",
    "This notebook demonstrates how to evaluate the agent with Mosaic AI Agent Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2689b1-33f2-48f7-9c5c-156e369dd32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster Configuration\n",
    "This notebook was tested on the following Databricks cluster configuration:\n",
    "- **Databricks Runtime Version:** 16.4 LTS ML (includes Apache Spark 3.5.2, Scala 2.12)\n",
    "- **Single Node** \n",
    "    - Azure: Standard_DS4_v2 (28 GB Memory, 8 Cores)\n",
    "    - AWS: m5d.2xlarge (32 GB Memory, 8 Cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be1a545-8c29-49b3-8397-adfa5ee5e4bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2ebb24-11d4-4b8c-ab7d-2b42b5ffcc21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4827460-0fe7-4ca1-a37a-dd718ae3ca86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks import agents\n",
    "from supply_chain_agent import config\n",
    "\n",
    "# Connect to the Unity catalog model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "catalog = config.get(\"catalog\")\n",
    "schema = config.get(\"schema\")\n",
    "agent_name = config.get(\"agent_name\")\n",
    "\n",
    "# Load the latest version of the model using pyfunc flavor\n",
    "agent = mlflow.pyfunc.load_model(f\"models:/{catalog}.{schema}.{agent_name}@latest\")\n",
    "\n",
    "user_email = spark.sql('select current_user() as user').collect()[0]['user']  # User email address\n",
    "first_name = user_email.split('.')[0]                                         # User first name\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{user_email}/supply-chain-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac92b163-47a9-42f0-9751-5097253cf846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55cc23c-90ab-4018-a8c7-f3f82b237d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from dbruntime.databricks_repl_context import get_context\n",
    "\n",
    "# TODO: set WORKSPACE_URL manually if it cannot be inferred from the current notebook\n",
    "WORKSPACE_URL = None\n",
    "if WORKSPACE_URL is None:\n",
    "  workspace_url_hostname = get_context().browserHostName\n",
    "  assert workspace_url_hostname is not None, \"Unable to look up current workspace URL. This can happen if running against serverless compute. Manually set WORKSPACE_URL yourself above, or run this notebook against classic compute\"\n",
    "  WORKSPACE_URL = f\"https://{workspace_url_hostname}\"\n",
    "\n",
    "# TODO: set secret_scope_name and secret_key_name to access your PAT\n",
    "secret_scope = first_name\n",
    "secret_key = \"token\"\n",
    "\n",
    "os.environ[\"HOST\"] = WORKSPACE_URL\n",
    "os.environ[\"TOKEN\"] = dbutils.secrets.get(scope=secret_scope, key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c41b9b7-163a-43cd-a0ed-e9a975669675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from supply_chain_agent import AGENT\n",
    "\n",
    "agent.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What happens if T2_4 goes down and takes 6 weeks to recover? What should I do?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113253a0-eb7c-4bf8-ab6c-d10f601d19b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics. Evaluate your agent with one of our [predefined LLM scorers](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f24623-27e4-4715-90ef-3d36da3592f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow.genai\n",
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.entities import Trace, Feedback, SpanType\n",
    "from mlflow.genai.scorers import Guidelines, RelevanceToQuery, Safety, scorer\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134d3dec-e0f0-4888-a8b9-13c7583d799f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a sample evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69eefaaf-364d-42d9-8377-8f06b0a53d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation dataset\n",
    "eval_data = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"List all downstream production sites for the raw material supplied by T3_10, and include any related information about these sites.\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": [\"data_analysis_tool\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Tell me what happens if T2_8 is disrupted and requires 9 to recover. What should I do?\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": [\"optimization_tool\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What happens if T2_4 goes down and takes 6 weeks to recover? What are the recommendations?\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": [\"optimization_tool\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Tell me the demand for T1_5 and the inventory levels of all materials needed to produce this finished goods.\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": [\"data_analysis_tool\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"There has been an incident at T3_15 and it will go down for the next 10 time units. What can I do to mitigate the risk?\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_tool\": [\"optimization_tool\"],\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4bc25b-e915-49b0-b81a-4aa14288e94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will generate initial traces by running the agent. The results, including traces, are logged to the MLflow experiment defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0772c3f-fa06-48ad-b9e0-ff3eef742150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "def evaluate_model(messages) -> dict:\n",
    "    return agent.predict({\"messages\": messages})\n",
    "\n",
    "@scorer\n",
    "def dummy_metric():\n",
    "    # This scorer is just to help generate initial traces.\n",
    "    return 1\n",
    "  \n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=evaluate_model,\n",
    "    scorers=[dummy_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26976c08-e65f-41f8-ba8e-323c81e147b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Access data from the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b623a52-a5e8-4063-91de-e22fec922ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generated_traces = mlflow.search_traces(run_id=results.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cecb645-14e0-4f60-b79e-589f2138a21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define our own evaluation criteria and wrap as custom scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254490f8-e5f5-4fb1-a598-8619b9fea2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the judge function for the tool usage\n",
    "@scorer\n",
    "def tool_usage(inputs: str, expectations: dict[str, Any], trace: Trace) -> Feedback:\n",
    "    tool_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "    used_tools = [json.loads(s.to_dict()['attributes']['mlflow.spanOutputs'])['name'] for s in tool_spans]\n",
    "    expected_tools = expectations.get(\"expected_tool\")\n",
    "    expected_tools_unused = list(set(expected_tools) - set(used_tools))\n",
    "    if not expected_tools_unused:\n",
    "        return Feedback(value=\"yes\", rationale=\"All expected tools were used.\")\n",
    "    else:\n",
    "        return Feedback(value=\"no\", rationale=f\"Following expected tools were not used: {expected_tools_unused}\")\n",
    "\n",
    "# Judge prompt for the right tool parameter setting\n",
    "tool_parameter_prompt = \"\"\"\n",
    "Evaluate the trace and determine if the right parameters were passed to the tool given the question.\n",
    "\n",
    "Question:\n",
    "{{{{question}}}}\n",
    "\n",
    "Tool spans to evaluate:\n",
    "{{{{spans}}}}\n",
    "\n",
    "Return ONLY a valid JSON object with exactly these keys:\n",
    "- \"rationale\": short explanation (<=80 words)\n",
    "- \"result\": one of [[pass]], [[fail]]\n",
    "\n",
    "STRICT OUTPUT RULES:\n",
    "- Output a single JSON object.\n",
    "- No markdown, no code fences, no extra text before/after.\n",
    "- Example (copy the format exactly):\n",
    "{{\"rationale\":\"...\", \"result\":\"pass\"}}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Define the judge function for the tool parameter setting\n",
    "@scorer\n",
    "def tool_parameter(trace: Trace) -> Feedback:\n",
    "    question = trace.search_spans(span_type=SpanType.AGENT)[0].inputs['messages'][0]['content']\n",
    "    tool_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "    tool_spans_str = \"\\n\".join(\n",
    "        json.dumps(s.to_dict(), ensure_ascii=False) for s in tool_spans\n",
    "    )\n",
    "    judge = custom_prompt_judge(\n",
    "        name=\"tool_parameter\",\n",
    "        prompt_template=tool_parameter_prompt,\n",
    "        numeric_values={\n",
    "            \"pass\": 1,\n",
    "            \"fail\": 0,\n",
    "        },\n",
    "    )\n",
    "    return judge(question=question, spans=tool_spans_str)\n",
    " \n",
    "# Define the judge function for the response time\n",
    "@scorer\n",
    "def response_time(trace: Trace) -> Feedback:\n",
    "    \n",
    "    # Search particular span type from the trace\n",
    "    agent_span = trace.search_spans(span_type=SpanType.AGENT)[0]\n",
    "\n",
    "    response_time = (agent_span.end_time_ns - agent_span.start_time_ns) / 1e9 # second\n",
    "    max_duration = 120\n",
    "    if response_time <= max_duration:\n",
    "        return Feedback(\n",
    "            value=\"yes\",\n",
    "            rationale=f\"Response time {response_time:.2f}s is within the {max_duration}s limit.\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=\"no\",\n",
    "            rationale=f\"Response time {response_time:.2f}s exceeds the {max_duration}s limit.\"\n",
    "        )\n",
    "\n",
    "# Define scorers\n",
    "scorers = [\n",
    "    Guidelines(\n",
    "        name=\"response_length\",\n",
    "        guidelines=\"The response MUST be concise and to the point not longer than 500 words.\",\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"professional_tone\",\n",
    "        guidelines=\"The response MUST be in a professional tone.\",\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"includes_recommendations\",\n",
    "        guidelines=\"The response MUST include specific, actionable recommendations.\",\n",
    "    ),\n",
    "    RelevanceToQuery(),\n",
    "    response_time,\n",
    "    tool_usage,\n",
    "    tool_parameter\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e36f2a-2b2a-473d-a45c-876486290433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run `mlflow.genai.evaluate` to apply the custom scoreres defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ff4465-584a-4735-b0de-9eae49a0ea16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\n",
    "trace_check_eval_results = mlflow.genai.evaluate(\n",
    "    data=generated_traces,\n",
    "    scorers=scorers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dcf429-9d1f-4889-b541-af17c4549d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assign `production` alias to this version of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f860dd-9a2b-4f41-b148-0466e255f499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_info = client.get_model_version_by_alias(f\"{catalog}.{schema}.{agent_name}\", \"latest\")\n",
    "client.set_registered_model_alias(f\"{catalog}.{schema}.{agent_name}\", \"production\", model_info.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c30f92-d9b3-48fa-9994-d5702e7b86eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "In this notebook, we explored how to evaluate the agent using custom metrics. In the next notebook, we will deploy the agent in Model Serving. See you there!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_evaluate_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
